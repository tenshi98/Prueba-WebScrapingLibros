# üìö Web Scraping - Books to Scrape (Standalone)

Proyecto completo de web scraping en Python para extraer informaci√≥n de libros desde [books.toscrape.com](https://books.toscrape.com) utilizando **Selenium con Chromium**, dise√±ado para **ejecuci√≥n standalone** sin necesidad de instalar dependencias v√≠a pip.

## üéØ Caracter√≠sticas

- ‚úÖ **Ejecuci√≥n standalone**: Usa dependencias del sistema (no requiere `pip install`)
- ‚úÖ **Chromium**: Usa chromedriver del sistema
- ‚úÖ **Extracci√≥n inteligente**:
  - Info b√°sica de **3 p√°ginas** del cat√°logo
  - Detalles completos de **5 libros** (descripci√≥n, UPC, categor√≠a)
- ‚úÖ Almacenamiento en base de datos SQLite
- ‚úÖ Detecci√≥n autom√°tica de duplicados por UPC y por Titulo
- ‚úÖ Rate limiting para no sobrecargar el servidor
- ‚úÖ Sistema de logging completo (INFO, WARNING, ERROR)
- ‚úÖ Manejo robusto de errores y reintentos
- ‚úÖ C√≥digo modular y bien documentado

## üìã Requisitos del Sistema

### Obligatorios

- **Python 3.10 o superior**
- **Chromium** (navegador)
- **ChromeDriver** (driver de Selenium para Chromium)
- **Selenium** (librer√≠a Python)

### Instalaci√≥n de Dependencias del Sistema

#### Ubuntu/Debian

```bash
# Instalar Chromium y ChromeDriver
sudo apt update
sudo apt install chromium-browser chromium-chromedriver

# Instalar Selenium a nivel de sistema
sudo apt install python3-selenium
```

#### Fedora/RHEL

```bash
# Instalar Chromium y ChromeDriver
sudo dnf install chromium chromedriver

# Instalar Selenium
sudo dnf install python3-selenium
```

#### Arch Linux

```bash
# Instalar Chromium y ChromeDriver
sudo pacman -S chromium chromedriver

# Instalar Selenium
sudo pacman -S python-selenium
```

#### Verificar Instalaci√≥n

```bash
# Verificar Chromium
chromium-browser --version

# Verificar ChromeDriver
chromedriver --version

# Verificar Selenium
python3 -c "import selenium; print(selenium.__version__)"
```

## üöÄ Instalaci√≥n del Proyecto

### 1. Clonar o descargar el proyecto

```bash
git clone https://github.com/tenshi98/Prueba-WebScrapingLibros.git
```

### 2. Verificar estructura

```bash
ls -la
```

Deber√≠as ver:
```
Prueba-WebScrapingLibros/
‚îú‚îÄ‚îÄ database/
‚îú‚îÄ‚îÄ scraper/
‚îú‚îÄ‚îÄ utils/
‚îú‚îÄ‚îÄ logs/
‚îú‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

### 3. (Opcional) Instalar Selenium v√≠a pip

Si no tienes Selenium a nivel de sistema, puedes instalarlo localmente:

```bash
pip3 install --user selenium
```

## ‚öôÔ∏è Configuraci√≥n

El archivo `config.py` contiene todas las configuraciones:

| Par√°metro | Valor por defecto | Descripci√≥n |
|-----------|-------------------|-------------|
| `MAX_PAGES` | 3 | N√∫mero de p√°ginas a extraer |
| `DETAIL_BOOKS_LIMIT` | 5 | Libros con detalles completos |
| `REQUEST_DELAY` | 2 segundos | Delay entre requests |
| `CHROMIUM_DRIVER_PATH` | `/usr/bin/chromedriver` | Path al chromedriver |
| `HEADLESS_MODE` | True | Ejecutar sin interfaz gr√°fica |

### Ajustar Path de ChromeDriver

Si tu chromedriver est√° en otra ubicaci√≥n, edita `config.py`:

```python
# Ubicaciones comunes:
CHROMIUM_DRIVER_PATH = '/usr/bin/chromedriver'           # Ubuntu/Debian
CHROMIUM_DRIVER_PATH = '/usr/local/bin/chromedriver'     # macOS/Fedora
CHROMIUM_DRIVER_PATH = 'chromedriver'                    # Si est√° en PATH
```

## üéÆ Uso

### Ejecuci√≥n B√°sica (Standalone)

```bash
python3 main.py
```

**No requiere activar entorno virtual ni instalar dependencias v√≠a pip** (si ya tienes Selenium en el sistema).

### Salida Esperada

```
================================================================================
Iniciando proceso de web scraping - Books to Scrape (Standalone)
================================================================================
2025-11-21 13:15:00 - books_scraper - INFO - Inicializando base de datos...
2025-11-21 13:15:00 - books_scraper - INFO - Libros en base de datos antes del scraping: 0
2025-11-21 13:15:00 - books_scraper - INFO - Inicializando scraper con Chromium...
2025-11-21 13:15:01 - books_scraper - INFO - Usando chromedriver en: /usr/bin/chromedriver
2025-11-21 13:15:02 - books_scraper - INFO - WebDriver de Chromium inicializado correctamente
2025-11-21 13:15:02 - books_scraper - INFO - Estrategia: Info b√°sica de 3 p√°ginas + detalles completos de 5 libros
2025-11-21 13:15:02 - books_scraper - INFO - Procesando p√°gina 1/3
...
================================================================================
Proceso de scraping completado
================================================================================
Libros extra√≠dos: 60
Libros insertados: 60
Libros con detalles completos: 5
Duplicados: 0
Errores: 0
Total en base de datos: 60
================================================================================
```

## üìÅ Estructura del Proyecto

```
Prueba-WebScrapingLibros/
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ db_manager.py          # Gesti√≥n de base de datos SQLite
‚îú‚îÄ‚îÄ scraper/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ book_scraper.py        # Scraper con Selenium + Chromium
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ logger.py              # Configuraci√≥n de logging
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ scraper.log            # Archivo de logs (generado autom√°ticamente)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ libros.db              # Base de datos SQLite (generado autom√°ticamente)
‚îú‚îÄ‚îÄ config.py                  # Configuraci√≥n centralizada
‚îú‚îÄ‚îÄ main.py                    # Script principal de ejecuci√≥n
‚îú‚îÄ‚îÄ requirements.txt           # Dependencias (solo selenium)
‚îî‚îÄ‚îÄ README.md                  # Este archivo
```

## üóÑÔ∏è Esquema de Base de Datos

```sql
CREATE TABLE libros (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    titulo TEXT NOT NULL,
    precio DECIMAL(10,2),
    disponibilidad TEXT,
    rating INTEGER,
    url_imagen TEXT,
    descripcion TEXT,
    upc TEXT UNIQUE,
    categoria TEXT,
    fecha_extraccion TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Datos Extra√≠dos

#### De todas las p√°ginas (info b√°sica):
- ‚úÖ T√≠tulo del libro
- ‚úÖ Precio
- ‚úÖ Disponibilidad (In stock/Out of stock)
- ‚úÖ Rating (cantidad de estrellas 1-5)
- ‚úÖ URL de la imagen de portada

#### Solo de los primeros 5 libros (detalles completos):
- ‚úÖ Descripci√≥n del producto
- ‚úÖ UPC (c√≥digo √∫nico)
- ‚úÖ Categor√≠a

### Consultar Datos

```bash
# Ver total de libros
sqlite3 data/libros.db "SELECT COUNT(*) FROM libros;"

# Ver libros con detalles completos
sqlite3 data/libros.db "SELECT titulo, descripcion IS NOT NULL as tiene_desc, upc FROM libros LIMIT 10;"

# Ver los 5 libros con detalles completos
sqlite3 data/libros.db "SELECT titulo, categoria, upc FROM libros WHERE descripcion IS NOT NULL;"

# Ver libros por categor√≠a
sqlite3 data/libros.db "SELECT categoria, COUNT(*) FROM libros WHERE categoria IS NOT NULL GROUP BY categoria;"
```

## üì¶ M√≥dulos

### `config.py`
Configuraci√≥n centralizada con:
- Rutas de archivos y directorios
- URLs del sitio web
- **Path a chromedriver de Chromium**
- **L√≠mite de libros con detalles completos (5)**
- Par√°metros de scraping (delays, timeouts)
- Configuraci√≥n de Selenium

### `database/db_manager.py`
Gesti√≥n de SQLite:
- `create_table()`: Crea la tabla si no existe
- `book_exists(upc)`: Verifica duplicados por UPC
- `insert_book(book_data)`: Inserta libro evitando duplicados
- `get_book_count()`: Obtiene total de libros

### `scraper/book_scraper.py`
Scraper standalone con Chromium:
- `setup_driver()`: Configura Chromium WebDriver del sistema
- `extract_books_from_page()`: Extrae info b√°sica o completa seg√∫n par√°metros
- `extract_book_details()`: Navega a p√°gina de detalle y extrae descripci√≥n, UPC, categor√≠a
- `scrape_books()`: Ejecuta extracci√≥n completa con l√≥gica de l√≠mite de detalles

### `utils/logger.py`
Sistema de logging:
- Logs en archivo (`logs/scraper.log`) y consola
- Niveles: INFO, WARNING, ERROR

### `main.py`
Script principal:
- Orquesta scraper y base de datos
- Muestra estad√≠sticas detalladas
- Maneja errores y cierre graceful

## üîç Funcionalidades Implementadas

### ‚úÖ Extracci√≥n Inteligente

El scraper usa una estrategia optimizada:

1. **P√°ginas 1-3**: Extrae info b√°sica de todos los libros (~60 libros)
2. **Primeros 5 libros**: Navega a p√°gina de detalle y extrae descripci√≥n, UPC, categor√≠a
3. **Resto de libros**: Solo info b√°sica (sin navegar a detalles)

Esto reduce el tiempo de ejecuci√≥n mientras cumple con el requisito de extraer detalles de al menos 5 libros.

### ‚úÖ Detecci√≥n de Duplicados

El sistema implementa una **validaci√≥n inteligente de duplicados** con dos niveles:

1. **Prioridad: Validaci√≥n por UPC**
   - Si el libro tiene UPC, se verifica por este c√≥digo √∫nico
   - M√°s confiable y preciso

2. **Fallback: Validaci√≥n por T√≠tulo**
   - Si el libro NO tiene UPC, se valida por el t√≠tulo
   - Evita duplicados incluso cuando falta informaci√≥n de UPC

**Ejemplo:**
```python
# Libro con UPC: valida por UPC
libro1 = {'titulo': 'Python Basics', 'upc': 'ABC123', ...}
libro2 = {'titulo': 'Python Basics (Edici√≥n 2)', 'upc': 'ABC123', ...}
# ‚ùå Duplicado detectado por UPC (aunque t√≠tulos sean diferentes)

# Libro sin UPC: valida por t√≠tulo
libro3 = {'titulo': 'JavaScript Guide', 'upc': None, ...}
libro4 = {'titulo': 'JavaScript Guide', 'upc': None, ...}
# ‚ùå Duplicado detectado por t√≠tulo
```

### ‚úÖ Rate Limiting
- 2 segundos de delay entre requests
- Evita sobrecargar el servidor
- Configurable en `config.py`

### ‚úÖ Manejo de Errores
- Reintentos autom√°ticos (3 intentos por p√°gina)
- Logging detallado de errores
- Continuaci√≥n del proceso ante errores individuales

### ‚úÖ Logging Completo
- **INFO**: Operaciones normales y progreso
- **WARNING**: Situaciones an√≥malas no cr√≠ticas
- **ERROR**: Errores que requieren atenci√≥n

## üêõ Troubleshooting

### Error: "chromedriver not found"

```bash
# Verificar si est√° instalado
which chromedriver

# Si no est√°, instalar:
sudo apt install chromium-chromedriver  # Ubuntu/Debian
```

### Error: "No module named 'selenium'"

```bash
# Instalar a nivel de sistema
sudo apt install python3-selenium

# O instalar para el usuario
pip3 install --user selenium
```

### Error: "Chrome binary not found"

```bash
# Instalar Chromium
sudo apt install chromium-browser
```

### Chromium en modo visible (no headless)

Edita `config.py`:
```python
HEADLESS_MODE = False
```

### Cambiar n√∫mero de libros con detalles

Edita `config.py`:
```python
DETAIL_BOOKS_LIMIT = 10  # Extraer detalles de 10 libros
```

## üìä Ejemplos de Uso

### Ver estad√≠sticas

```bash
# Total de libros
sqlite3 data/libros.db "SELECT COUNT(*) FROM libros;"

# Libros con detalles completos
sqlite3 data/libros.db "SELECT COUNT(*) FROM libros WHERE descripcion IS NOT NULL;"

# Precio promedio
sqlite3 data/libros.db "SELECT AVG(precio) FROM libros;"

# Distribuci√≥n de ratings
sqlite3 data/libros.db "SELECT rating, COUNT(*) FROM libros GROUP BY rating;"
```

### Exportar a CSV

```bash
sqlite3 -header -csv data/libros.db "SELECT * FROM libros;" > libros.csv
```

### Ver logs

```bash
cat logs/scraper.log
```

## üìù Notas Importantes

> [!IMPORTANT]
> **Ejecuci√≥n Standalone**: Este proyecto est√° dise√±ado para ejecutarse sin `pip install` usando dependencias del sistema. Aseg√∫rate de tener Chromium, chromedriver y python3-selenium instalados a nivel de sistema.

> [!NOTE]
> **Extracci√≥n Selectiva**: Solo los primeros 5 libros tendr√°n descripci√≥n, UPC y categor√≠a completos. El resto solo tendr√° informaci√≥n b√°sica (t√≠tulo, precio, rating, disponibilidad, imagen).

> [!TIP]
> Si quieres extraer detalles de m√°s libros, modifica `DETAIL_BOOKS_LIMIT` en `config.py`.

## ‚ö†Ô∏è Disclaimer

Este proyecto es solo para fines educativos. Aseg√∫rate de revisar y respetar los t√©rminos de servicio del sitio web que est√°s scrapeando.

---

**Desarrollado con Python 3.10+ y Selenium + Chromium** üêç‚ú®
